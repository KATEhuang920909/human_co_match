# -*- coding: utf-8 -*-"""@Time    : 2021/6/1 11:05@Author  : huangkai21@file    : basic_feature_extract.py"""# ! -*- coding: utf-8 -*-# 测试代码可用性: 提取特征"""将vocab里的每一个字转化为向量形式，保存为json字典"""import numpy as npfrom bert4keras.backend import kerasfrom bert4keras.models import build_transformer_modelfrom bert4keras.tokenizers import Tokenizer# from bert4keras.snippets import to_arraydef to_array(*args):    """批量转numpy的array    """    results = [np.array(a) for a in args]    if len(args) == 1:        return results[0]    else:        return resultsconfig_path = r'E:\code\chinese_L-12_H-768_A-12/bert_config.json'checkpoint_path = r'E:\code\chinese_L-12_H-768_A-12/bert_model.ckpt'dict_path = r'E:\code\chinese_L-12_H-768_A-12/vocab.txt'tokenizer = Tokenizer(dict_path, do_lower_case=True)  # 建立分词器model = build_transformer_model(config_path, checkpoint_path)  # 建立模型，加载权重# def bert_encoder(vocab_bag):#     for string in txt:#         token_ids, segment_ids = tokenizer.encode(string)#         token_ids, segment_ids = to_array([token_ids], [segment_ids])#         result = model.predict([token_ids, segment_ids])#         result = result[0][1:-1]# 编码测试# token_ids, segment_ids = tokenizer.encode(u'语言模型')# token_ids, segment_ids = to_array([token_ids], [segment_ids])def bert_encoder(char_list, ):    token_ids, segment_ids = tokenizer.encode(char_list)    token_ids, segment_ids = to_array([token_ids], [segment_ids])    result = model.predict([token_ids, segment_ids])    return result[0]    # np.save(f"../saved/{corpus_name}.npy", corpus_dic)if __name__ == '__main__':    # with open(dict_path, encoding='utf-8')  as f:    #     char = f.readlines()    #     char = [cr.strip() for cr in char]    print(bert_encoder(["你", "是", "谁"]))    # from sklearn.metrics.pairwise import cosine_similarity    #    # #    # txt1 = ["词", "语", "言"]    # txt2 = "词"    # # txt3 = u"语义"    # token_ids1, segment_ids1 = tokenizer.encode(txt1)    # token_ids2, segment_ids2 = tokenizer.encode(txt2)    # # token_ids3, segment_ids3 = tokenizer.encode(txt3)    # #    # token_ids1, segment_ids1 = to_array([token_ids1], [segment_ids1])    # token_ids2, segment_ids2 = to_array([token_ids2], [segment_ids2])    # # token_ids3, segment_ids3 = to_array([token_ids3], [segment_ids3])    # #    # # print('\n ===== predicting =====\n')    # print(token_ids1, token_ids2)    # result1 = model.predict([token_ids1, segment_ids1])    # result2 = model.predict([token_ids2, segment_ids2])    # # result3 = model.predict([token_ids3, segment_ids3])    # #    # # v1 = np.sum(result1[0][1:2], axis=0)    # # v2 = np.sum(result1[1][2:3], axis=0)    # # v3 = np.sum(result3[0][1:3], axis=0)    # print(result1, result2)    # # print(cosine_similarity(result1[0][0].reshape(1, -1), result1[0][1].reshape(1, -1)))    # print(cosine_similarity(result1[0][0].reshape(1, -1), result1[0][1].reshape(1, -1)))    # print(cosine_similarity(result1[0][1].reshape(1, -1), result1[0][2].reshape(1, -1)))    # # # print(cosine_similarity(result1[0][-1].reshape(1, -1), result2[0][-1].reshape(1, -1)))    """    输出：    [[[-0.63251007  0.2030236   0.07936534 ...  0.49122632 -0.20493352        0.2575253 ]      [-0.7588351   0.09651865  1.0718756  ... -0.6109694   0.04312154        0.03881441]      [ 0.5477043  -0.792117    0.44435206 ...  0.42449304  0.41105673        0.08222899]      [-0.2924238   0.6052722   0.49968526 ...  0.8604137  -0.6533166        0.5369075 ]      [-0.7473459   0.49431565  0.7185162  ...  0.3848612  -0.74090636        0.39056838]      [-0.8741375  -0.21650358  1.338839   ...  0.5816864  -0.4373226        0.56181806]]]    """    # print('\n ===== reloading and predicting =====\n')    # model.save('test.model')    # del model    # model = keras.models.load_model('test.model')    # print(model.predict([token_ids, segment_ids]))